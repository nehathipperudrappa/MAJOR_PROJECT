{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Majorproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehathipperudrappa/MAJOR_PROJECT/blob/main/Majorproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YuAVrYcaql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81125c18-366b-4755-9185-011b581b7729"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9c58qlZlETs"
      },
      "source": [
        "df = pd.read_csv('/content/Review_data.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPG0qChNbbok"
      },
      "source": [
        "df['rating'] = df['rating'].replace(5,'Good')\n",
        "df['rating'] = df['rating'].replace(4,'Good')\n",
        "df['rating'] = df['rating'].replace(3,'Good')\n",
        "df['rating'] = df['rating'].replace(2,'Bad')\n",
        "df['rating'] = df['rating'].replace(1,'Bad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWMA0llieqJ4"
      },
      "source": [
        "#preprocess data\n",
        "df['review'] = df['review'].str.lower()  #Convert to lower case\n",
        "df['review'] = df['review'].str.replace('\\d+', '') #removing numbers\n",
        "df['review'] = df['review'].str.replace('.','')\n",
        "df['review'] = df['review'].str.replace('-','')\n",
        "df['review'] = df['review'].str.replace('?','')\n",
        "df['review'] = df['review'].str.replace(\"'\",'')\n",
        "df['review'] = df['review'].str.replace('  ','')  #for extra spaces\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzmnOfeiigH"
      },
      "source": [
        "#tokenization and stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "df['stemmed'] = df['tokenized_sents'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "df = df.drop(columns=['tokenized_sents'])\n",
        "df['review'] = df['stemmed'].apply(lambda x: ' '.join(map(str, x)))\n",
        "df = df.drop(columns=['stemmed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kZcxZChm_Pk"
      },
      "source": [
        "x = df.iloc[:,0]\n",
        "y = df.iloc[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6GYvcThnN2U"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3dgJKq0kdGm"
      },
      "source": [
        "#Vectorization and model fitting using pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "text_model = Pipeline([('tfidi',TfidfVectorizer()),('model',SVC())])\n",
        "text_model.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEfjc4OWk0N-"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "y_pred = text_model.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YivqMo_VoPkM"
      },
      "source": [
        "#confusion matrix\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmpsde01odSE"
      },
      "source": [
        "#Classification report\n",
        "classification_report(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_I_trZaokfB"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(text_model,'Email_Class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmPVJE4spZbU"
      },
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvnuJb2Tp134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9696e5dd-a5ac-4537-8709-bbee239bcd60"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import sklearn\n",
        "import joblib\n",
        "model =joblib.load('Email_Class')\n",
        "\n",
        "add_selectbox = st.sidebar.radio(\"Our page\",['Home',\"Rate us\"])\n",
        "if(add_selectbox == 'Home'):\n",
        "  st.title(\"Review classifier\")\n",
        "  st.image('https://static.businessworld.in/article/article_extra_large_image/1600858456_HulSrV_Flipkart.jpg')\n",
        "\n",
        "  ip = st.text_input('Enter your review')\n",
        "  p = model.predict([ip])\n",
        "  if st.button('Predict'):\n",
        "    st.header(op[0])\n",
        "    if(op[0]=='Good'):\n",
        "      st.markdown(\"\"\":smile:\"\"\")\n",
        "    else:\n",
        "      st.markdown(\"\"\":angry:\"\"\")\n",
        "else:\n",
        "  st.title(\"Rate US\")\n",
        "  st.slider(\"Rating\",0,5)\n",
        "  if st.button(\"Submit\"):\n",
        "    st.header(\"Thanks for the rating\")\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A2eRnFwruAC"
      },
      "source": [
        "!nohup streamlit run app.py&\n",
        "url = ngrok.connect(port = '8501')\n",
        "url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36wLkK5ryB2"
      },
      "source": [
        "!pip install pipreqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xxnRefv9NUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d333e03a-06fd-4da7-e478-c2dee94bf7f1"
      },
      "source": [
        "!pipreqs /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Successfully saved requirements file in /content/requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP8rE1NTI19r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}